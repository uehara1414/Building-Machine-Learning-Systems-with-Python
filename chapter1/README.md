## メモ

sum() 関数は True = 1, False=0 として加算する


一定の列のみの配列に分割するには
`a[:,0]`
`a[:,2]`
などとする


「~」を使うことで、配列の要素に対して論理否定演算を行える
```python
x = x[~sp.isnan(y)]
```
上記コードでxのそれぞれの要素に対して、対応するyがnanでないもののみxに残せる


直線で近似するには　Scipy の polyfit() を使う
x, yと多項式の次元を指定すると誤差を最小とするモデル関数を得られる。

与えられたデータへの過度の適応を過学習という。

様々な視点からデータを眺めることが大事。
今回は単純な多項式で近似しようとしたが、それでうまくモデルができていないならば別のアプローチを取る必要がある。

今回で言えば、以下の三つのアプローチがある
- 多項式モデルのうち、どれか一つを選択する
- より複雑なモデルに切り換える
- データを別の視点から捉え直し、初めからやりなおす

今回のデータは３周目と４周目との間に急に変化しているから、そこを界に直線で近似してみる。

しかし直線モデルだとまだ誤差が多項式モデルより大きい。

未来のデータを取得できれば、そのデータとの近似誤差からモデルを選定できる。
しかし、未来のデータは取得できないので、一部データをテスト用に利用することで同様なことをシミュレーションできる。

その方法は、全データの数パーセントを保持しておき、残りデータを使って学習をするというもの。
このとき、テスト用に保持（ホールドアウト）しておいたデータをホールドアウトデータという。
この方法により「学習モデルが新しいデータについてどう振る舞うか」といった、より現実的な視点で評価できる。
